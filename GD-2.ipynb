{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75eb8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/destination_folder/nsmc-master/ratings_corpus.txt --model_prefix=spm_model --vocab_size=10000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/destination_folder/nsmc-master/ratings_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/destination_folder/nsmc-master/ratings_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 150000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5473088\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1709\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 150000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 308899 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 150000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 366536\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 366536 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=159076 obj=15.6055 num_tokens=868459 num_tokens/piece=5.4594\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=147387 obj=14.5242 num_tokens=873556 num_tokens/piece=5.92695\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=110481 obj=14.6225 num_tokens=909487 num_tokens/piece=8.23207\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=110309 obj=14.5668 num_tokens=910216 num_tokens/piece=8.25151\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=82730 obj=14.8043 num_tokens=954833 num_tokens/piece=11.5416\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82723 obj=14.7421 num_tokens=954974 num_tokens/piece=11.5442\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62042 obj=15.0026 num_tokens=997401 num_tokens/piece=16.0762\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62042 obj=14.9414 num_tokens=997477 num_tokens/piece=16.0774\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46531 obj=15.2323 num_tokens=1042908 num_tokens/piece=22.4132\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46531 obj=15.1675 num_tokens=1042940 num_tokens/piece=22.4139\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34898 obj=15.4925 num_tokens=1089707 num_tokens/piece=31.2255\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34898 obj=15.4253 num_tokens=1089769 num_tokens/piece=31.2273\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26173 obj=15.7826 num_tokens=1138322 num_tokens/piece=43.4922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26173 obj=15.7095 num_tokens=1138337 num_tokens/piece=43.4928\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19629 obj=16.0997 num_tokens=1189127 num_tokens/piece=60.5801\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19629 obj=16.0189 num_tokens=1189149 num_tokens/piece=60.5812\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14721 obj=16.4432 num_tokens=1243668 num_tokens/piece=84.4826\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14721 obj=16.351 num_tokens=1243683 num_tokens/piece=84.4836\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11040 obj=16.8211 num_tokens=1301967 num_tokens/piece=117.932\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11040 obj=16.7101 num_tokens=1302273 num_tokens/piece=117.96\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11000 obj=16.7147 num_tokens=1303448 num_tokens/piece=118.495\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11000 obj=16.713 num_tokens=1303453 num_tokens/piece=118.496\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_model.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm_model.vocab\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# 데이터 로드\n",
    "trn = pd.read_csv('/aiffel/aiffel/destination_folder/nsmc-master/ratings_train.txt', sep='\\t', quoting=3)\n",
    "tst = pd.read_csv('/aiffel/aiffel/destination_folder/nsmc-master/ratings_test.txt', sep='\\t', quoting=3)\n",
    "\n",
    "# SentencePiece 모델 학습\n",
    "corpus_file = '/aiffel/aiffel/destination_folder/nsmc-master/ratings_corpus.txt'\n",
    "trn['document'].to_csv(corpus_file, index=False, header=False)\n",
    "spm.SentencePieceTrainer.train('--input={} --model_prefix=spm_model --vocab_size=10000'.format(corpus_file))\n",
    "\n",
    "# SentencePiece 모델 로드\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "# 데이터 전처리\n",
    "trn['document'] = trn['document'].astype(str)\n",
    "tst['document'] = tst['document'].astype(str)\n",
    "\n",
    "trn_text = trn['document'].apply(lambda x: sp.encode_as_ids(x)).tolist()\n",
    "tst_text = tst['document'].apply(lambda x: sp.encode_as_ids(x)).tolist()\n",
    "\n",
    "trn_text = pad_sequences(trn_text, padding='post')\n",
    "tst_text = pad_sequences(tst_text, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47196db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2344/2344 [==============================] - 21s 8ms/step - loss: 0.6932 - accuracy: 0.5012\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 19s 8ms/step - loss: 0.6932 - accuracy: 0.4995\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 19s 8ms/step - loss: 0.6932 - accuracy: 0.5020\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 20s 8ms/step - loss: 0.6932 - accuracy: 0.5005\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 20s 8ms/step - loss: 0.6932 - accuracy: 0.4994\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6932 - accuracy: 0.4965\n",
      "Test Loss: 0.6931642889976501\n",
      "Test Accuracy: 0.49654000997543335\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "embedding_dim = 128\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(sp), embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(trn_text, trn['label'], epochs=5, batch_size=64)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(tst_text, tst['label'], verbose=1)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e03ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data - First 5 rows:\n",
      "         id                                           document  label\n",
      "0   9976970                               아 더 빙 진짜 짜증 나 네요 목소리      0\n",
      "1   3819312                흠 포스터 보고 초딩 영화 줄 오버 연기 조차 가볍 지 않 구나      1\n",
      "2  10265843                                너무 재 밓었다그래서보는것을추천한다      0\n",
      "3   9045019                      교도소 이야기 구먼 솔직히 재미 는 없 다 평점 조정      0\n",
      "4   6483659  사이몬페그 의 익살 스런 연기 가 돋보였 던 영화 스파이더맨 에서 늙 어 보이 기 ...      1\n",
      "\n",
      "Test Data - First 5 rows:\n",
      "        id                                           document  label\n",
      "0  6270596                                                굳 ㅋ      1\n",
      "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2  8544678           뭐 야 평점 들 은 나쁘 진 않 지만 10 점 짜리 는 더더욱 아니 잖아      0\n",
      "3  6825595                지루 하 지 는 않 은데 완전 막장 임 돈 주 고 보 기 에 는      0\n",
      "4  6723715  3 D 만 아니 었 어도 별 다섯 개 줬 을 텐데 왜 3 D 로 나와서 제 심기 를...      0\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.3776 - accuracy: 0.8301 - val_loss: 0.3310 - val_accuracy: 0.8563\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2934 - accuracy: 0.8746 - val_loss: 0.3199 - val_accuracy: 0.8618\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2512 - accuracy: 0.8945 - val_loss: 0.3394 - val_accuracy: 0.8604\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2146 - accuracy: 0.9118 - val_loss: 0.3507 - val_accuracy: 0.8570\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1808 - accuracy: 0.9270 - val_loss: 0.3943 - val_accuracy: 0.8539\n",
      "1563/1563 - 3s - loss: 0.3969 - accuracy: 0.8515\n",
      "Test Accuracy: 0.8514599800109863\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 데이터 로드\n",
    "trn = pd.read_csv('/aiffel/aiffel/destination_folder/nsmc-master/ratings_train.txt', sep='\\t', quoting=3)\n",
    "tst = pd.read_csv('/aiffel/aiffel/destination_folder/nsmc-master/ratings_test.txt', sep='\\t', quoting=3)\n",
    "\n",
    "# 특수 문자 및 불필요한 문자 제거\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9a-zA-Z\\\\s]\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 불용어 제거\n",
    "def remove_stopwords(text, stopwords):\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# 정규화\n",
    "def normalize(text, tokenizer):\n",
    "    tokens = tokenizer.morphs(text)\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# 불용어 리스트 생성\n",
    "stopwords = ['은', '는', '이', '가', '을', '를']\n",
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# 데이터 전처리\n",
    "trn['document'] = trn['document'].apply(lambda x: clean_text(str(x)))\n",
    "trn['document'] = trn['document'].apply(lambda x: remove_stopwords(x, stopwords))\n",
    "trn['document'] = trn['document'].apply(lambda x: normalize(x, tokenizer))\n",
    "\n",
    "tst['document'] = tst['document'].apply(lambda x: clean_text(str(x)))\n",
    "tst['document'] = tst['document'].apply(lambda x: remove_stopwords(x, stopwords))\n",
    "tst['document'] = tst['document'].apply(lambda x: normalize(x, tokenizer))\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"Train Data - First 5 rows:\")\n",
    "print(trn.head())\n",
    "\n",
    "print(\"\\nTest Data - First 5 rows:\")\n",
    "print(tst.head())\n",
    "\n",
    "# 정수 인코딩\n",
    "vocab_size = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(trn['document'])\n",
    "\n",
    "trn_text = tokenizer.texts_to_sequences(trn['document'])\n",
    "tst_text = tokenizer.texts_to_sequences(tst['document'])\n",
    "\n",
    "# 패딩\n",
    "max_len = 30\n",
    "trn_text = pad_sequences(trn_text, maxlen=max_len)\n",
    "tst_text = pad_sequences(tst_text, maxlen=max_len)\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(trn_text, trn['label'], epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(tst_text, tst['label'], verbose=2)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
